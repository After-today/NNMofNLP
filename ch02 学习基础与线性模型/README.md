
# 第二章 学习基础与线性模型

推荐阅读：Daume III[2015]、 Shalev Shwartz 和 Ben-David[2014]和 Mohri 等人[2012]。

<!-- TOC -->

- [第二章 学习基础与线性模型](#第二章-学习基础与线性模型)
- [1. 有监督学习和参数化函数](#1-有监督学习和参数化函数)
- [2. 训练集、测试集和验证集](#2-训练集测试集和验证集)
- [3. 线性模型](#3-线性模型)
    - [3.1. 二分类](#31-二分类)
    - [3.2. 对数线性二分类](#32-对数线性二分类)
    - [3.3. 多分类](#33-多分类)
- [4. 表示](#4-表示)
- [5. 独热和稠密向量表示](#5-独热和稠密向量表示)
- [6. 对数线性多分类](#6-对数线性多分类)
- [7. 训练和最优化](#7-训练和最优化)
    - [7.1. 损失函数](#71-损失函数)
    - [7.2. 正则化](#72-正则化)
- [8. 基于梯度的最优化](#8-基于梯度的最优化)
    - [8.1. 随机梯度下降](#81-随机梯度下降)
    - [8.2. 实例](#82-实例)
    - [8.3. 其他训练方法](#83-其他训练方法)

<!-- /TOC -->

# 1. 有监督学习和参数化函数

- 函数簇————假设类
因为搜索所有可能的程序（或函数）是非常困难（和极其不明确）的问题，通常把函数限制在特定的函数簇内，比如所有的具有$d_{in}$个输入、$d_{out}$个输出的线性函数所组成的函数空间，或者所有包含$d_{in}$个变量的决策树空间 。  
<br>
- 归纳偏置
一组关于期望结果形式的假设，同时也使得搜索结果的过程更加高效

- 常见的假设类——高维线形函数
$$f(x)=x \cdot W+b$$

# 2. 训练集、测试集和验证集

# 3. 线性模型

## 3.1. 二分类

## 3.2. 对数线性二分类

## 3.3. 多分类

# 4. 表示

# 5. 独热和稠密向量表示

# 6. 对数线性多分类

# 7. 训练和最优化

## 7.1. 损失函数

## 7.2. 正则化

# 8. 基于梯度的最优化

## 8.1. 随机梯度下降

## 8.2. 实例

## 8.3. 其他训练方法
